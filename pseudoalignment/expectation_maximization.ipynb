{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudoalignment and Expectation Maximization of Transcript Counts\n",
    "\n",
    "Aligned reads cannot neccessarily be easily assigned to transcripts. This is due to ambiguous alignment that occurrs very frequently. In practice only `30%` of reads align uniquely to a single transcript. The main reason for this is the fact that most genes have multiple transcripts that share a significant number of exons. Furthermore, genes from the same gene families share significant sequence similarity. The figure below shows an example of a gene with two transcripts and reads that align to different regions of the gene causing `ambiguous` and `unique` matches.\n",
    "\n",
    "![Alignment ambiguity](images/alignment.jpg)\n",
    "\n",
    "Reads in purple can come from either `transcript 1` or `transcript 2`. Some regions however are unique. This is especially true for unique splice junctions, where two exons are fused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Read Alignment Assignment\n",
    "\n",
    "In an naive assignment we can split the count attribution between all matching transcripts. For an alignment matching transcripts `[3,4,5]` each transcript will receive a third of a read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed alignments\n",
    "aligned_reads = [[0,1], [0,3], [3,4,5], [4,5]]\n",
    "\n",
    "counts = [0]*6\n",
    "\n",
    "for a in aligned_reads:\n",
    "    for i in a:\n",
    "        counts[i] = counts[i] + 1/len(a)\n",
    "\n",
    "print(counts)\n",
    "sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization\n",
    "\n",
    "Instead of splitting the alignment equally, we can split the counts relative to the transcript abundance. For this we first have to define an initial state. In this example all transcripts have the same probability `counts = [1]*6`. None of the initial values should be 0 (they will stay 0 otherwise).\n",
    "\n",
    "We will now iteratively update the probabilities (In this example `100` times), by looping over the alignments and plitting counts relative to their previous probabilities. After each iteration we replace the transcript probabilities by the newly found counts (probabilities do not need to be normalized). \n",
    "\n",
    "We iterate until there are only very small changes and we converged to a global maximum (it can be proven that the solution is not local but a global)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_reads = [[0,1], [0,3], [3,4,5], [4,5]]\n",
    "\n",
    "counts = [1]*6\n",
    "temp_counts = [0]*6\n",
    "intermediate_counts = []\n",
    "\n",
    "for k in range(100):\n",
    "    temp_counts = [0]*6\n",
    "    for a in aligned_reads:\n",
    "\n",
    "        sum_counts = 0\n",
    "        for i in a:\n",
    "            sum_counts = sum_counts + counts[i]\n",
    "\n",
    "        for i in a:\n",
    "            temp_counts[i] = temp_counts[i] + counts[i]/sum_counts\n",
    "    counts = temp_counts.copy()\n",
    "    intermediate_counts.append(counts)\n",
    "sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Estimated Transcript Counts\n",
    "\n",
    "Here we can plot the expected counts over the `100` iterations we computed. We can observe that the EM algorithm quickly converges to a solution for all transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = 0\n",
    "\n",
    "df = pd.DataFrame(intermediate_counts)\n",
    "plt.plot(df.iloc[:,transcript], linewidth=4)\n",
    "plt.xlabel(\"interation\", fontsize=16)\n",
    "plt.ylabel(\"counts\", fontsize=16)\n",
    "tx = plt.text(100, np.max(df.iloc[:,transcript]), \"transcript \"+str(transcript), fontsize=20, verticalalignment='top', horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of EM Behavior\n",
    "\n",
    "Transcript counts can look quite different with small changes in our alignment data. If we add an unambiguous alignment for transcript `3` the EM algorithm will weigh transcript 3 as much more probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_reads = [[0,1], [0,3], [3,4,5], [4,5], [3]]\n",
    "\n",
    "counts = [1]*6\n",
    "temp_counts = [0]*6\n",
    "intermediate_counts_2 = []\n",
    "\n",
    "for k in range(100):\n",
    "    temp_counts = [0]*6\n",
    "    for a in aligned_reads:\n",
    "\n",
    "        sum_counts = 0\n",
    "        for i in a:\n",
    "            sum_counts = sum_counts + counts[i]\n",
    "\n",
    "        for i in a:\n",
    "            temp_counts[i] = temp_counts[i] + counts[i]/sum_counts\n",
    "    counts = temp_counts.copy()\n",
    "    \n",
    "    intermediate_counts_2.append(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = 3\n",
    "\n",
    "df = pd.DataFrame(intermediate_counts)\n",
    "plt.plot(df.iloc[:,transcript], linewidth=4, label=\"ambiguous 3\")\n",
    "\n",
    "df = pd.DataFrame(intermediate_counts_2)\n",
    "plt.plot(df.iloc[:,transcript], linewidth=4, label=\"unambiguous 3\")\n",
    "\n",
    "plt.xlabel(\"interation\", fontsize=16)\n",
    "plt.ylabel(\"counts\", fontsize=16)\n",
    "tx = plt.text(100, np.max(df.iloc[:,transcript]), \"transcript \"+str(transcript), fontsize=20, verticalalignment='top', horizontalalignment='right')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization with Transcript Length Correction\n",
    "\n",
    "The length of transcripts matters when trying to associate reads to transcripts. Given that two genes are expressed at exactly the same level, but one transcript has a length of `2000bp` while the second has a length of only `500bp`. Assuming all things equal we would expect to find about `4` times as many reads of the longer transcript.\n",
    "\n",
    "We can correct for such differences by multiplying the transcript length to the transcript probability. There are slightly more complicationed methods such as effective length, which take fragment length into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_reads = [[0,1], [0,3], [3,4,5], [4,5]]\n",
    "\n",
    "transcript_length = [100, 150, 400, 300, 50, 120]\n",
    "\n",
    "counts = [1]*6\n",
    "temp_counts = [0]*6\n",
    "intermediate_counts_length = []\n",
    "\n",
    "for k in range(100):\n",
    "    temp_counts = [0]*6\n",
    "    for a in aligned_reads:\n",
    "\n",
    "        sum_counts = 0\n",
    "        for i in a:\n",
    "            sum_counts = sum_counts + counts[i]*transcript_length[i]\n",
    "\n",
    "        for i in a:\n",
    "            temp_counts[i] = temp_counts[i] + (counts[i]*transcript_length[i])/sum_counts\n",
    "    counts = temp_counts.copy()\n",
    "    \n",
    "    intermediate_counts_length.append(counts)\n",
    "sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = 0\n",
    "\n",
    "df = pd.DataFrame(intermediate_counts)\n",
    "plt.plot(df.iloc[:,transcript], linewidth=4, label=\"em\")\n",
    "\n",
    "df = pd.DataFrame(intermediate_counts_length)\n",
    "plt.plot(df.iloc[:,transcript], linewidth=4, label=\"em + length\")\n",
    "\n",
    "plt.xlabel(\"interation\", fontsize=16)\n",
    "plt.ylabel(\"counts\", fontsize=16)\n",
    "tx = plt.text(100, np.max(df.iloc[:,transcript]), \"transcript \"+str(transcript), fontsize=20, verticalalignment='top', horizontalalignment='right')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.bar(np.array(range(len(intermediate_counts[-1])))-0.2, intermediate_counts[-1], 0.4, label=\"em\")\n",
    "p = plt.bar(np.array(range(len(intermediate_counts_length[-1])))+0.2, intermediate_counts_length[-1], 0.4, label=\"em+length\")\n",
    "ll = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complicated Corrections (GC Content Example)\n",
    "\n",
    " There are many ways the reads could be biased, with transcript length being the most straigh forward (length of transcript affects probability linearly). You can add as many corrections to the model presented before as you like and can think of. \n",
    "\n",
    "Medium to high GC bias seems to be more commonly found in Illumina reads (https://pubmed.ncbi.nlm.nih.gov/22323520/). Similarly to transcript length we can modify the count splitting with the corresponding bias weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_reads = [[0,1], [0,3], [3,4,5], [4,5]]\n",
    "\n",
    "transcript_length = [100, 150, 400, 300, 50, 120]\n",
    "\n",
    "gc_content_bias = [1.2, 0.8, 1, 1.4, 1.1, 0.7]\n",
    "\n",
    "counts = [1]*6\n",
    "temp_counts = [0]*6\n",
    "intermediate_counts_length_gc = []\n",
    "\n",
    "for k in range(100):\n",
    "    temp_counts = [0]*6\n",
    "    for a in aligned_reads:\n",
    "\n",
    "        sum_counts = 0\n",
    "        for i in a:\n",
    "            sum_counts = sum_counts + counts[i]*transcript_length[i]*gc_content_bias[i]\n",
    "\n",
    "        for i in a:\n",
    "            temp_counts[i] = temp_counts[i] + (counts[i]*transcript_length[i]*gc_content_bias[i])/sum_counts\n",
    "    counts = temp_counts.copy()\n",
    "    \n",
    "    intermediate_counts_length_gc.append(counts)\n",
    "sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.bar(np.array(range(len(intermediate_counts_length[-1])))-0.2, intermediate_counts_length[-1], 0.4, label=\"em+length\")\n",
    "p = plt.bar(np.array(range(len(intermediate_counts_length_gc[-1])))+0.2, intermediate_counts_length_gc[-1], 0.4, label=\"em+length+GC\")\n",
    "ll = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
